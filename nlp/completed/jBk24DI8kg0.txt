{"text":"{|4180|Every NLP task needs to do text normalization:|}{|4900|/n/1.Segmenting or tokenizing words in running text|}{|19860|/n/2.Normalizing word formats|}{|19861|/n/3.Segmenting sentences in running text|}{|22680|/n/ /b/How many words in this sentence?/ |}{|35200|/n/Complicated question - fragments, filled pauses|}{|51720|/n/Pluralization - lemmas|}{|73980|/n/Wordform - full inflected surface form|}{|78220|/n/ /b/Another example sentence/ |}{|97440|/n/Words can be defined as type and token|}{|162980|/n/N = Number of tokens|}{|177260|/n/V = vocabulary = set of types |}{|232080|/n/Corpus size of phone conversations in English, Shakespeare and Google N-grams|}{|239100|/n/ /b/Standard Unix tools used for text processing/ |}{|301160|/n/Extracting all words in corpus: tr -sc |}{|350460|/n/Sort the words extracted|}{|374780|/n/Program unique will take sorted file and tell for each unique type the count|}{|413660|/n/Sort the word list in frequency order|}{|456020|/n/Fixing the case|}{|509680|/n/We have another problem - letters like d and s|}{|525720|/n/Issues in tokenization|}{|591060|/n/Periods can become a huge issue|}{|600340|/n/ /b/Tokenization - language issues/ |}{|617240|/n/Other languages like French and German are even more complex for tokenization|}{|649280|/n/ /b/Tokenization - Chinese and Japanese languages/ |}{|691600|/n/Japanese has different alphabets interleaved|}{|702900|/n/ /b/Word tokenization in Chinese/ |}{|714160|/n/Word segmentation problem|}{|738200|/n/ /b/Maximum matching word segmentation algorithm/ |}{|743140|/n/Given a word list of Chinese, and a string|}{|761201|/n/ /b/Max-match segmentation illustration/ |}{|774101|/n/thecatinthehat|}{|802622|/n/the phrase thetabledownthere|}{|854043|/n/Max-match does not work well in English, but works very well in Chinese|}{|860323|/n/Modern probabilistic segmentation algorithms work even better|}","css":"","videoid":"jBk24DI8kg0","title":"2 - 3 - Word Tokenization- Stanford NLP - Professor Dan Jurafsky & Chris Manning","duration":865}